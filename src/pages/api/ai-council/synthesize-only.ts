/**
 * Synthesize-only endpoint for AI Council
 * Allows synthesizing pre-collected responses without running new queries
 */

import type { APIRoute } from 'astro';

export const prerender = false;

// Environment variables
const OPENAI_API_KEY = import.meta.env.OPENAI_API_KEY;
const ANTHROPIC_API_KEY = import.meta.env.ANTHROPIC_API_KEY;
const GOOGLE_AI_API_KEY = import.meta.env.GOOGLE_AI_API_KEY;
const XAI_API_KEY = import.meta.env.XAI_API_KEY;

type SynthesisModel = 'claude' | 'claude-opus' | 'openai' | 'gpt4o' | 'gemini' | 'grok';

interface TokenUsage {
  inputTokens: number;
  outputTokens: number;
}

interface CostInfo {
  inputCost: number;
  outputCost: number;
  totalCost: number;
}

interface AIResponse {
  model: string;
  provider: string;
  response: string;
  error?: string;
  duration: number;
  tokens?: TokenUsage;
  cost?: CostInfo;
}

// Cost calculation
const MODEL_PRICING: Record<string, { input: number; output: number }> = {
  'gpt-4o': { input: 2.5, output: 10 },
  'claude-sonnet-4-20250514': { input: 3, output: 15 },
  'claude-opus-4-20250514': { input: 15, output: 75 },
  'gemini-2.0-flash': { input: 0.1, output: 0.4 },
  'grok-4': { input: 3, output: 15 },
};

function calculateCost(model: string, tokens: TokenUsage): CostInfo {
  const pricing = MODEL_PRICING[model] || { input: 0, output: 0 };
  const inputCost = (tokens.inputTokens / 1_000_000) * pricing.input;
  const outputCost = (tokens.outputTokens / 1_000_000) * pricing.output;
  return { inputCost, outputCost, totalCost: inputCost + outputCost };
}

// Build synthesis prompt
function buildSynthesisPrompt(originalPrompt: string, responses: AIResponse[]): string {
  const validResponses = responses.filter(r => !r.error && r.response);
  const modelCount = validResponses.length;
  
  return `Du √§r en expertsyntetiserare. Analysera f√∂ljande AI-modellers svar p√• samma fr√•ga och skapa EN sammanh√§ngande, f√∂rb√§ttrad syntes.

## Originalfr√•ga:
${originalPrompt}

## AI-modellernas svar (${modelCount} modeller):

${validResponses.map(r => `### ${r.provider}:
${r.response}
`).join('\n---\n\n')}

## Din uppgift:

**B√ñRJA ALLTID med en Konsensusanalys:**

\`\`\`
üìä KONSENSUSANALYS
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
√ñverensst√§mmelse: [H√ñG/MEDEL/L√ÖG] - [kort f√∂rklaring]

‚úÖ Alla modeller √∂verens om:
‚Ä¢ [punkt 1]
‚Ä¢ [punkt 2]

‚ö†Ô∏è Konflikter/skillnader:
‚Ä¢ [vad de √§r oeniga om och vilka modeller]

üí° Unika insikter (endast en modell):
‚Ä¢ [modell]: [insikt] ‚Üê Verifiera denna!
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
\`\`\`

**Sedan skapa en SYNTES som:**
1. Kombinerar de b√§sta insikterna fr√•n alla svar
2. Tydligt markerar om n√•got ENDAST kommer fr√•n en modell (potentiell hallucination)
3. Eliminerar redundans och mots√§gelser
4. √Ñr v√§lstrukturerad och l√§tt att l√§sa
5. Anv√§nder markdown f√∂r formatering
6. √Ñr p√• svenska

## Avsluta ALLTID med en "Cursor Implementation Guide":

\`\`\`markdown
## Cursor Implementation Guide

### Filer att skapa/√§ndra
- [ ] fil1.ts - Beskrivning av √§ndring
- [ ] fil2.ts - Beskrivning av √§ndring

### Steg-f√∂r-steg
1. F√∂rsta steget...
2. Andra steget...

### Kodexempel (om relevant)
[Inkludera konkreta kodexempel som kan kopieras direkt]
\`\`\`

VIKTIGT: Skriv ENDAST konsensusanalysen, syntesen och Implementation Guide, ingen meta-kommentar om processen.`;
}

// Build super-synthesis prompt (after deliberation)
function buildSuperSynthesisPrompt(
  originalPrompt: string, 
  round1Responses: AIResponse[], 
  round2Responses: AIResponse[]
): string {
  const validRound1 = round1Responses.filter(r => !r.error && r.response);
  const validRound2 = round2Responses.filter(r => !r.error && r.response);
  
  return `Du √§r en senior teknisk expert och arkitekt. Din uppgift √§r att skapa en SUPERSYNTES baserad p√• en tv√•stegsprocess d√§r AI-modeller f√∂rst svarat individuellt, sedan granskat varandras svar.

## Originalfr√•ga:
${originalPrompt}

## RUNDA 1 - Initiala svar:

${validRound1.map(r => `### ${r.provider} (${r.model}):
${r.response}
`).join('\n---\n\n')}

## RUNDA 2 - Granskning och f√∂rb√§ttring:

${validRound2.map(r => `### ${r.provider} (${r.model}) - F√∂rb√§ttrat svar:
${r.response}
`).join('\n---\n\n')}

## Din uppgift (Supersyntes):

**STEG 1: Extrahera konfliktdata fr√•n Runda 2**

Runda 2-svaren inneh√•ller strukturerade block med:
- \`\`\`konflikt\`\`\` - identifierade konflikter (MOTS√ÑGELSE, UNIK_INSIKT, UTAN_K√ÑLLA, M√ñJLIG_HALLUCINATION)
- \`\`\`l√∂sning\`\`\` - f√∂reslagna l√∂sningar med s√§kerhetsgrad

**Samla alla unika konflikter och l√∂sningar fr√•n alla modellers Runda 2-svar.**

**STEG 2: Skapa Konsensusanalys**

\`\`\`
üìä KONSENSUSANALYS (efter riktad faktagranskning)
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
√ñverensst√§mmelse: [H√ñG/MEDEL/L√ÖG] - [kort f√∂rklaring]

üîç Identifierade konflikter i Runda 2:
‚Ä¢ MOTS√ÑGELSER: [antal] st
‚Ä¢ UNIKA INSIKTER (hallucinationsrisk): [antal] st  
‚Ä¢ P√ÖST√ÖENDEN UTAN K√ÑLLA: [antal] st

üîÑ L√∂sta konflikter:
‚Ä¢ [konflikt] ‚Üí [l√∂sning] (s√§kerhet: H√ñG/MEDEL/L√ÖG)

‚ö†Ô∏è OL√ñSTA konflikter (kr√§ver manuell verifiering):
‚Ä¢ [konflikt som modellerna inte kunde l√∂sa]

‚úÖ Slutgiltig konsensus (alla modeller √∂verens efter granskning):
‚Ä¢ [punkt 1]
‚Ä¢ [punkt 2]

‚ùå F√∂rkastade p√•st√•enden (hallucinationer/fel som korrigerats):
‚Ä¢ [modell]: "[p√•st√•ende]" - FELAKTIGT pga [anledning]
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
\`\`\`

**STEG 3: Skapa supersyntesen**
1. **Basera p√• konsensus**: Inkludera ENDAST p√•st√•enden som f√•tt st√∂d efter granskning
2. **Markera os√§kerheter**: Anv√§nd ‚ö†Ô∏è f√∂r saker som inte kunde verifieras
3. **Exkludera hallucinationer**: Utel√§mna f√∂rkastade p√•st√•enden helt
4. **Slutgiltig rekommendation**: Ge en definitiv, v√§lgrundad rekommendation

## Avsluta ALLTID med en "Cursor Implementation Guide":

\`\`\`markdown
## Cursor Implementation Guide

### Filer att skapa/√§ndra
- [ ] fil1.ts - Beskrivning av √§ndring
- [ ] fil2.ts - Beskrivning av √§ndring

### Steg-f√∂r-steg
1. F√∂rsta steget...
2. Andra steget...

### Kodexempel (om relevant)
[Inkludera konkreta kodexempel som kan kopieras direkt]
\`\`\`

Skriv din supersyntes p√• svenska. Var extra noggrann med att lyfta fram vad som korrigerades mellan rundorna. Implementation Guide ska vara praktisk och direkt anv√§ndbar i Cursor.`;
}

// Synthesis functions for each model
async function synthesizeWithClaude(prompt: string, responses: AIResponse[]): Promise<AIResponse> {
  const start = Date.now();
  try {
    const synthesisPrompt = buildSynthesisPrompt(prompt, responses);
    
    const response = await fetch('https://api.anthropic.com/v1/messages', {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        'x-api-key': ANTHROPIC_API_KEY,
        'anthropic-version': '2023-06-01',
      },
      body: JSON.stringify({
        model: 'claude-sonnet-4-20250514',
        max_tokens: 8192,
        messages: [{ role: 'user', content: synthesisPrompt }],
      }),
    });

    if (!response.ok) {
      const error = await response.json();
      throw new Error(error.error?.message || `HTTP ${response.status}`);
    }

    const data = await response.json();
    const tokens: TokenUsage = {
      inputTokens: data.usage?.input_tokens || 0,
      outputTokens: data.usage?.output_tokens || 0,
    };
    
    return {
      model: 'claude-sonnet-4-20250514',
      provider: 'Claude Sonnet (Syntes)',
      response: data.content?.[0]?.text || 'Syntes misslyckades',
      duration: Date.now() - start,
      tokens,
      cost: calculateCost('claude-sonnet-4-20250514', tokens),
    };
  } catch (error: any) {
    return {
      model: 'claude-sonnet-4-20250514',
      provider: 'Claude Sonnet (Syntes)',
      response: '',
      error: error.message,
      duration: Date.now() - start,
    };
  }
}

async function synthesizeWithClaudeOpus(prompt: string, responses: AIResponse[]): Promise<AIResponse> {
  const start = Date.now();
  try {
    const synthesisPrompt = buildSynthesisPrompt(prompt, responses);
    
    const response = await fetch('https://api.anthropic.com/v1/messages', {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        'x-api-key': ANTHROPIC_API_KEY,
        'anthropic-version': '2023-06-01',
      },
      body: JSON.stringify({
        model: 'claude-opus-4-20250514',
        max_tokens: 8192,
        messages: [{ role: 'user', content: synthesisPrompt }],
      }),
    });

    if (!response.ok) {
      const error = await response.json();
      throw new Error(error.error?.message || `HTTP ${response.status}`);
    }

    const data = await response.json();
    const tokens: TokenUsage = {
      inputTokens: data.usage?.input_tokens || 0,
      outputTokens: data.usage?.output_tokens || 0,
    };
    
    return {
      model: 'claude-opus-4-20250514',
      provider: 'Claude Opus 4.5 (Syntes)',
      response: data.content?.[0]?.text || 'Syntes misslyckades',
      duration: Date.now() - start,
      tokens,
      cost: calculateCost('claude-opus-4-20250514', tokens),
    };
  } catch (error: any) {
    return {
      model: 'claude-opus-4-20250514',
      provider: 'Claude Opus 4.5 (Syntes)',
      response: '',
      error: error.message,
      duration: Date.now() - start,
    };
  }
}

async function synthesizeWithGPT4o(prompt: string, responses: AIResponse[]): Promise<AIResponse> {
  const start = Date.now();
  try {
    const synthesisPrompt = buildSynthesisPrompt(prompt, responses);
    
    const response = await fetch('https://api.openai.com/v1/chat/completions', {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        'Authorization': `Bearer ${OPENAI_API_KEY}`,
      },
      body: JSON.stringify({
        model: 'gpt-4o',
        messages: [{ role: 'user', content: synthesisPrompt }],
      }),
    });

    if (!response.ok) {
      const error = await response.json();
      throw new Error(error.error?.message || `HTTP ${response.status}`);
    }

    const data = await response.json();
    const tokens: TokenUsage = {
      inputTokens: data.usage?.prompt_tokens || 0,
      outputTokens: data.usage?.completion_tokens || 0,
    };
    
    return {
      model: 'gpt-4o',
      provider: 'GPT-4o (Syntes)',
      response: data.choices[0]?.message?.content || 'Syntes misslyckades',
      duration: Date.now() - start,
      tokens,
      cost: calculateCost('gpt-4o', tokens),
    };
  } catch (error: any) {
    return {
      model: 'gpt-4o',
      provider: 'GPT-4o (Syntes)',
      response: '',
      error: error.message,
      duration: Date.now() - start,
    };
  }
}

async function synthesizeWithGemini(prompt: string, responses: AIResponse[]): Promise<AIResponse> {
  const start = Date.now();
  try {
    const synthesisPrompt = buildSynthesisPrompt(prompt, responses);
    
    const response = await fetch(
      `https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key=${GOOGLE_AI_API_KEY}`,
      {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({
          contents: [{ parts: [{ text: synthesisPrompt }] }],
          generationConfig: { maxOutputTokens: 8192 },
        }),
      }
    );

    if (!response.ok) {
      const error = await response.json();
      throw new Error(error.error?.message || `HTTP ${response.status}`);
    }

    const data = await response.json();
    const tokens: TokenUsage = {
      inputTokens: data.usageMetadata?.promptTokenCount || 0,
      outputTokens: data.usageMetadata?.candidatesTokenCount || 0,
    };
    
    return {
      model: 'gemini-2.0-flash',
      provider: 'Gemini (Syntes)',
      response: data.candidates?.[0]?.content?.parts?.[0]?.text || 'Syntes misslyckades',
      duration: Date.now() - start,
      tokens,
      cost: calculateCost('gemini-2.0-flash', tokens),
    };
  } catch (error: any) {
    return {
      model: 'gemini-2.0-flash',
      provider: 'Gemini (Syntes)',
      response: '',
      error: error.message,
      duration: Date.now() - start,
    };
  }
}

async function synthesizeWithGrok(prompt: string, responses: AIResponse[]): Promise<AIResponse> {
  const start = Date.now();
  try {
    const synthesisPrompt = buildSynthesisPrompt(prompt, responses);
    
    const response = await fetch('https://api.x.ai/v1/chat/completions', {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        'Authorization': `Bearer ${XAI_API_KEY}`,
      },
      body: JSON.stringify({
        model: 'grok-4',
        messages: [{ role: 'user', content: synthesisPrompt }],
        max_tokens: 8192,
      }),
    });

    if (!response.ok) {
      const error = await response.json();
      throw new Error(error.error?.message || `HTTP ${response.status}`);
    }

    const data = await response.json();
    const tokens: TokenUsage = {
      inputTokens: data.usage?.prompt_tokens || 0,
      outputTokens: data.usage?.completion_tokens || 0,
    };
    
    return {
      model: 'grok-4',
      provider: 'Grok (Syntes)',
      response: data.choices[0]?.message?.content || 'Syntes misslyckades',
      duration: Date.now() - start,
      tokens,
      cost: calculateCost('grok-4', tokens),
    };
  } catch (error: any) {
    return {
      model: 'grok-4',
      provider: 'Grok (Syntes)',
      response: '',
      error: error.message,
      duration: Date.now() - start,
    };
  }
}

// Main synthesis function
async function synthesize(
  prompt: string,
  responses: AIResponse[],
  synthesisModel: SynthesisModel = 'claude'
): Promise<AIResponse> {
  switch (synthesisModel) {
    case 'gpt4o':
      return synthesizeWithGPT4o(prompt, responses);
    case 'gemini':
      return synthesizeWithGemini(prompt, responses);
    case 'grok':
      return synthesizeWithGrok(prompt, responses);
    case 'claude-opus':
      return synthesizeWithClaudeOpus(prompt, responses);
    case 'openai':
      return synthesizeWithGPT4o(prompt, responses); // Use GPT-4o for "openai" since o1 is slow
    case 'claude':
    default:
      return synthesizeWithClaude(prompt, responses);
  }
}

// Super-synthesis function (uses the same model but with super-synthesis prompt)
async function superSynthesize(
  prompt: string,
  round1Responses: AIResponse[],
  round2Responses: AIResponse[],
  synthesisModel: SynthesisModel = 'claude'
): Promise<AIResponse> {
  // Use the appropriate model for super synthesis
  // Note: We're passing the super prompt directly since it already includes all context
  switch (synthesisModel) {
    case 'gpt4o':
      return synthesizeWithGPT4oSuper(prompt, round1Responses, round2Responses);
    case 'gemini':
      return synthesizeWithGeminiSuper(prompt, round1Responses, round2Responses);
    case 'grok':
      return synthesizeWithGrokSuper(prompt, round1Responses, round2Responses);
    case 'claude-opus':
      return synthesizeWithClaudeOpusSuper(prompt, round1Responses, round2Responses);
    case 'openai':
      return synthesizeWithGPT4oSuper(prompt, round1Responses, round2Responses);
    case 'claude':
    default:
      return synthesizeWithClaudeSuper(prompt, round1Responses, round2Responses);
  }
}

// Super synthesis variants that use the super-synthesis prompt
async function synthesizeWithClaudeSuper(prompt: string, r1: AIResponse[], r2: AIResponse[]): Promise<AIResponse> {
  const start = Date.now();
  try {
    const superPrompt = buildSuperSynthesisPrompt(prompt, r1, r2);
    
    const response = await fetch('https://api.anthropic.com/v1/messages', {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        'x-api-key': ANTHROPIC_API_KEY,
        'anthropic-version': '2023-06-01',
      },
      body: JSON.stringify({
        model: 'claude-sonnet-4-20250514',
        max_tokens: 8192,
        messages: [{ role: 'user', content: superPrompt }],
      }),
    });

    if (!response.ok) {
      const error = await response.json();
      throw new Error(error.error?.message || `HTTP ${response.status}`);
    }

    const data = await response.json();
    const tokens: TokenUsage = {
      inputTokens: data.usage?.input_tokens || 0,
      outputTokens: data.usage?.output_tokens || 0,
    };
    
    return {
      model: 'claude-sonnet-4-20250514',
      provider: 'Claude Sonnet (Supersyntes)',
      response: data.content?.[0]?.text || 'Supersyntes misslyckades',
      duration: Date.now() - start,
      tokens,
      cost: calculateCost('claude-sonnet-4-20250514', tokens),
    };
  } catch (error: any) {
    return {
      model: 'claude-sonnet-4-20250514',
      provider: 'Claude Sonnet (Supersyntes)',
      response: '',
      error: error.message,
      duration: Date.now() - start,
    };
  }
}

async function synthesizeWithClaudeOpusSuper(prompt: string, r1: AIResponse[], r2: AIResponse[]): Promise<AIResponse> {
  const start = Date.now();
  try {
    const superPrompt = buildSuperSynthesisPrompt(prompt, r1, r2);
    
    const response = await fetch('https://api.anthropic.com/v1/messages', {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        'x-api-key': ANTHROPIC_API_KEY,
        'anthropic-version': '2023-06-01',
      },
      body: JSON.stringify({
        model: 'claude-opus-4-20250514',
        max_tokens: 8192,
        messages: [{ role: 'user', content: superPrompt }],
      }),
    });

    if (!response.ok) {
      const error = await response.json();
      throw new Error(error.error?.message || `HTTP ${response.status}`);
    }

    const data = await response.json();
    const tokens: TokenUsage = {
      inputTokens: data.usage?.input_tokens || 0,
      outputTokens: data.usage?.output_tokens || 0,
    };
    
    return {
      model: 'claude-opus-4-20250514',
      provider: 'Claude Opus 4.5 (Supersyntes)',
      response: data.content?.[0]?.text || 'Supersyntes misslyckades',
      duration: Date.now() - start,
      tokens,
      cost: calculateCost('claude-opus-4-20250514', tokens),
    };
  } catch (error: any) {
    return {
      model: 'claude-opus-4-20250514',
      provider: 'Claude Opus 4.5 (Supersyntes)',
      response: '',
      error: error.message,
      duration: Date.now() - start,
    };
  }
}

async function synthesizeWithGPT4oSuper(prompt: string, r1: AIResponse[], r2: AIResponse[]): Promise<AIResponse> {
  const start = Date.now();
  try {
    const superPrompt = buildSuperSynthesisPrompt(prompt, r1, r2);
    
    const response = await fetch('https://api.openai.com/v1/chat/completions', {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        'Authorization': `Bearer ${OPENAI_API_KEY}`,
      },
      body: JSON.stringify({
        model: 'gpt-4o',
        messages: [{ role: 'user', content: superPrompt }],
      }),
    });

    if (!response.ok) {
      const error = await response.json();
      throw new Error(error.error?.message || `HTTP ${response.status}`);
    }

    const data = await response.json();
    const tokens: TokenUsage = {
      inputTokens: data.usage?.prompt_tokens || 0,
      outputTokens: data.usage?.completion_tokens || 0,
    };
    
    return {
      model: 'gpt-4o',
      provider: 'GPT-4o (Supersyntes)',
      response: data.choices[0]?.message?.content || 'Supersyntes misslyckades',
      duration: Date.now() - start,
      tokens,
      cost: calculateCost('gpt-4o', tokens),
    };
  } catch (error: any) {
    return {
      model: 'gpt-4o',
      provider: 'GPT-4o (Supersyntes)',
      response: '',
      error: error.message,
      duration: Date.now() - start,
    };
  }
}

async function synthesizeWithGeminiSuper(prompt: string, r1: AIResponse[], r2: AIResponse[]): Promise<AIResponse> {
  const start = Date.now();
  try {
    const superPrompt = buildSuperSynthesisPrompt(prompt, r1, r2);
    
    const response = await fetch(
      `https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key=${GOOGLE_AI_API_KEY}`,
      {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({
          contents: [{ parts: [{ text: superPrompt }] }],
          generationConfig: { maxOutputTokens: 8192 },
        }),
      }
    );

    if (!response.ok) {
      const error = await response.json();
      throw new Error(error.error?.message || `HTTP ${response.status}`);
    }

    const data = await response.json();
    const tokens: TokenUsage = {
      inputTokens: data.usageMetadata?.promptTokenCount || 0,
      outputTokens: data.usageMetadata?.candidatesTokenCount || 0,
    };
    
    return {
      model: 'gemini-2.0-flash',
      provider: 'Gemini (Supersyntes)',
      response: data.candidates?.[0]?.content?.parts?.[0]?.text || 'Supersyntes misslyckades',
      duration: Date.now() - start,
      tokens,
      cost: calculateCost('gemini-2.0-flash', tokens),
    };
  } catch (error: any) {
    return {
      model: 'gemini-2.0-flash',
      provider: 'Gemini (Supersyntes)',
      response: '',
      error: error.message,
      duration: Date.now() - start,
    };
  }
}

async function synthesizeWithGrokSuper(prompt: string, r1: AIResponse[], r2: AIResponse[]): Promise<AIResponse> {
  const start = Date.now();
  try {
    const superPrompt = buildSuperSynthesisPrompt(prompt, r1, r2);
    
    const response = await fetch('https://api.x.ai/v1/chat/completions', {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        'Authorization': `Bearer ${XAI_API_KEY}`,
      },
      body: JSON.stringify({
        model: 'grok-4',
        messages: [{ role: 'user', content: superPrompt }],
        max_tokens: 8192,
      }),
    });

    if (!response.ok) {
      const error = await response.json();
      throw new Error(error.error?.message || `HTTP ${response.status}`);
    }

    const data = await response.json();
    const tokens: TokenUsage = {
      inputTokens: data.usage?.prompt_tokens || 0,
      outputTokens: data.usage?.completion_tokens || 0,
    };
    
    return {
      model: 'grok-4',
      provider: 'Grok (Supersyntes)',
      response: data.choices[0]?.message?.content || 'Supersyntes misslyckades',
      duration: Date.now() - start,
      tokens,
      cost: calculateCost('grok-4', tokens),
    };
  } catch (error: any) {
    return {
      model: 'grok-4',
      provider: 'Grok (Supersyntes)',
      response: '',
      error: error.message,
      duration: Date.now() - start,
    };
  }
}

export const POST: APIRoute = async ({ request }) => {
  try {
    const body = await request.json();
    const { prompt, responses, round2Responses, synthesisModel = 'claude', isSuperSynthesis = false } = body;

    if (!prompt) {
      return new Response(JSON.stringify({ error: 'Prompt saknas' }), {
        status: 400,
        headers: { 'Content-Type': 'application/json' },
      });
    }

    if (!responses || responses.length === 0) {
      return new Response(JSON.stringify({ error: 'Inga svar att syntetisera' }), {
        status: 400,
        headers: { 'Content-Type': 'application/json' },
      });
    }

    let synthesis: AIResponse;
    
    // Use super-synthesis if R2 responses are provided
    if (isSuperSynthesis && round2Responses && round2Responses.length > 0) {
      synthesis = await superSynthesize(prompt, responses, round2Responses, synthesisModel);
    } else {
      synthesis = await synthesize(prompt, responses, synthesisModel);
    }

    return new Response(JSON.stringify({
      success: true,
      synthesis,
      responsesCount: responses.length,
      isSuperSynthesis: isSuperSynthesis && round2Responses && round2Responses.length > 0,
    }), {
      status: 200,
      headers: { 'Content-Type': 'application/json' },
    });

  } catch (error: any) {
    console.error('Synthesize-only error:', error);
    return new Response(JSON.stringify({ error: error.message || 'Ok√§nt fel' }), {
      status: 500,
      headers: { 'Content-Type': 'application/json' },
    });
  }
};
